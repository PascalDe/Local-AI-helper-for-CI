# SAP Integration Suite ‚Äì Local AI Error Helper ü§ñ

> A local AI prototype running on a Raspberry Pi 5 that helps you research and resolve errors encountered in the SAP Integration Suite.

---

## üí° Motivation

Working with the SAP Integration Suite means encountering error messages ‚Äì sometimes cryptic ones. The idea behind this project was simple: what if there were a local AI tool that could help research those errors, suggest root causes, and point towards solutions?

Requirements:
- Runs **locally** on minimal hardware (Raspberry Pi 5)
- **No licensing fees**
- Uses **SAP notes**, **SAP blog posts**, and a **personal error history** as knowledge sources

> **Disclaimer:** I am neither a professional software developer nor an AI expert. This project is a basic prototype intended to demonstrate that building a local AI application on minimal hardware is very much achievable.  
> SAP notes and blog posts used here are dummy examples generated by ChatGPT. To the best of my knowledge, there is currently no official API to automatically retrieve SAP note contents. Automating that process may violate SAP's Terms of Service, so I have intentionally avoided it.

---

## üèóÔ∏è Architecture

The overall flow looks like this:

```
[SAP BTP ‚Äì Cloud Integration]
        ‚Üì  (1) Error occurs
      [PC]
        ‚Üì  (2) Python UI / CURL sends error text
  [Raspberry Pi 5]
    ‚îú‚îÄ‚îÄ FastAPI endpoint: /analyze-error
    ‚îú‚îÄ‚îÄ MariaDB  ‚Üê previously known issues & solutions   (3)
    ‚îú‚îÄ‚îÄ ChromaDB ‚Üê SAP notes & SAP blog posts            (3)
    ‚îî‚îÄ‚îÄ TinyLlama (via llama.cpp)                        (4)
        ‚Üì
    Answer returned to PC
```

1. An error occurs while working on SAP Integration Suite (Cloud Integration / CPI).
2. A Python application on the PC sends the error message to the Raspberry Pi. Alternatively, a simple `curl` command can be used.
3. A FastAPI endpoint on the Raspberry Pi receives the message and queries both **MariaDB** (known issues) and **ChromaDB** (SAP notes & blog posts).
4. The retrieved context is forwarded to **TinyLlama** (running via llama.cpp), which generates an answer and returns it to the caller.

---

## üìÅ Repository Structure

```
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pi-scripts/
‚îÇ   ‚îú‚îÄ‚îÄ chromadb/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setup_chroma.py          # Creates the ChromaDB collection "SAP_solutions"
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ insert_sap_note.py       # Inserts a dummy SAP note
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ insert_sap_blog.py       # Inserts a dummy SAP blog post
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ check_chroma.py          # Verifies collection contents
‚îÇ   ‚îú‚îÄ‚îÄ fastapi/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ main.py              # FastAPI app with /health and /analyze-error endpoints
‚îÇ   ‚îî‚îÄ‚îÄ llm_model/
‚îÇ       ‚îî‚îÄ‚îÄ download_model.py        # Downloads TinyLlama from HuggingFace
‚îî‚îÄ‚îÄ llm-ui/
    ‚îú‚îÄ‚îÄ main.py                      # Python desktop UI for sending error messages
    ‚îî‚îÄ‚îÄ config.py                    # Configuration (set your Raspberry Pi IP here)
```

---

## üõ†Ô∏è Implementation

### Prerequisites

- Raspberry Pi 5 with a fresh install of **Raspberry Pi OS Lite**
- A PC for running the Python UI
- A free [Hugging Face](https://huggingface.co) account (for downloading the LLM model)

---

### Step 1 ‚Äì Initial Setup on the Raspberry Pi

Install required system packages and create the folder structure:

```bash
sudo apt install python3-full python3-pip python3-venv build-essential cmake
sudo apt install git libcurl4-openssl-dev -y

mkdir ci_project
mkdir ci_project/chroma
mkdir ci_project/shared_data
mkdir ci_project/llm_model
mkdir ci_project/fastapi
mkdir ci_project/fastapi/app
```

**Folder overview:**

| Folder | Purpose |
|---|---|
| `chroma` | ChromaDB setup and population scripts |
| `fastapi/app` | FastAPI application logic |
| `llama.cpp` | LLM inference engine |
| `llm_model` | The actual TinyLlama model file |
| `shared_data` | ChromaDB persistent storage |

---

### Step 2 ‚Äì ChromaDB Setup

Navigate into the chroma folder, create a virtual environment, and install ChromaDB:

```bash
cd ~/ci_project/chroma
python3 -m venv venv
source venv/bin/activate
pip install chromadb
```

Create the collection `SAP_solutions` and verify it works:

```bash
python3 setup_chroma.py
```

Expected output:
```
Inserted test entry
Collections: ['SAP_solutions']
Count: 1
Files: ['chroma.sqlite3', 'c92342b4-9a83-49e2-8669-96caa4592ba7']
```

Insert dummy data (SAP note and SAP blog post):

```bash
python3 insert_sap_note.py
python3 insert_sap_blog.py
```

Verify the entries:

```bash
python3 check_chroma.py
```

Expected output:
```
Collections: ['SAP_solutions']
Collection found
Count: 3
```

---

### Step 3 ‚Äì MariaDB Setup

Install MariaDB:

```bash
sudo apt install mariadb-server mariadb-client -y
systemctl status mariadb
```

Run the guided security setup:

```bash
mariadb-secure-installation
```

Log in and create the database and table:

```bash
mariadb -u [yourUser] -p
```

```sql
CREATE DATABASE ciDB;
USE ciDB;

CREATE TABLE errorTable (
  id                  BIGINT NOT NULL AUTO_INCREMENT,
  timestamp           DATETIME NOT NULL,
  system_type_sender  VARCHAR(100),
  system_type_receiver VARCHAR(100),
  error_code          VARCHAR(100),
  error_message       TEXT,
  stacktrace          TEXT,
  payload_snippet     TEXT,
  severity            ENUM('INFO','WARN','ERROR','CRITICAL'),
  resolution          TEXT,
  created_at          DATETIME DEFAULT CURRENT_TIMESTAMP,
  PRIMARY KEY (id)
);
```

Insert a dummy entry:

```sql
INSERT INTO errorTable
(`timestamp`, system_type_sender, system_type_receiver, error_code,
 error_message, stacktrace, payload_snippet, severity, resolution, created_at)
VALUES (
  '2026-01-14 10:30:00',
  'SAP_S4HANA',
  'SAP_CPI',
  'HTTP_403',
  'Received HTTP 403 Forbidden from target system',
  'com.sap.it.api.exception.HttpResponseException: HTTP 403 Forbidden\n at com.sap.it.adapter.http.HttpAdapter.send(HttpAdapter.java:123)\n at com.sap.it.integration.ProcessHandler.handle(ProcessHandler.java:45)',
  '{ "businessPartnerId": "4711", "action": "CREATE" }',
  'ERROR',
  'Check the permissions of the API user in the target system. Ensure that the user has the necessary roles to perform the CREATE action on the Business Partner. Also verify OAuth tokens or authentication parameters.',
  '2026-01-14 10:31:00'
);

EXIT
```

---

### Step 4 ‚Äì LLM Setup (llama.cpp + TinyLlama)

#### Build llama.cpp

```bash
cd ~/ci_project
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake .. -DLLAMA_NATIVE=ON
make -j4
```

> ‚è≥ This may take a few minutes!

#### Download TinyLlama

The model used is `tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf` ‚Äì a medium, balanced quality model from [Hugging Face](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF), licensed under **Apache 2.0**.

You will need a free Hugging Face account and an access token.

```bash
cd ~/ci_project/llm_model
python3 -m venv venv
source venv/bin/activate
pip3 install huggingface_hub

# Enter your Hugging Face token in download_model.py before running!
sudo nano download_model.py
python3 download_model.py
```

Copy the downloaded model into the models folder:

```bash
cp tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf ~/ci_project/llm_model/models
```

#### Test the model

```bash
cd ~/ci_project/llama.cpp/build/bin
./llama-cli -m ~/ci_project/llm_model/models/tinyllama-1.1b-chat-v0.3.Q4_K_M.gguf \
  -p "Hello LLM!" -n 256
```

---

### Step 5 ‚Äì FastAPI Setup

```bash
cd ~/ci_project/fastapi
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn
python3 -m pip install chromadb llama-cpp-python mysql-connector-python
```

> ‚è≥ This may take a while!

Start the server:

```bash
uvicorn app.main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 1 \
  --log-level info
```

Expected output:
```
INFO: Started server process [6433]
INFO: Waiting for application startup.
INFO: Application startup complete.
INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Available endpoints:**

| Endpoint | Method | Description |
|---|---|---|
| `/health` | GET | Check if the API is up and running |
| `/analyze-error` | POST | Send an error message and receive an AI-generated analysis |

---

## üß™ Testing

### Option A ‚Äì Quick test via CURL

```bash
curl -X POST http://[YOUR-RASPBERRY-PI-IP]:8000/analyze-error \
  -H "Content-Type: application/json" \
  -d '{
    "error_message": "HTTP 403 Forbidden in SAP CPI",
    "system_sender": "S4HANA",
    "system_receiver": "SAP_CPI"
  }'
```

### Option B ‚Äì Python Desktop UI

1. Open the `llm-ui` folder in VS Code
2. Set your Raspberry Pi's IP address in `config.py`
3. Run `main.py`
4. Enter an SAP error message and click **Analyse**
5. Wait for the LLM response (this may take a moment)

---

## ‚úÖ Conclusion

The result is a fully local, zero-cost AI prototype that can help diagnose SAP Integration Suite errors using a combination of:
- A personal **error history** (MariaDB)
- **SAP notes and blog posts** as reference material (ChromaDB)
- A **locally running LLM** (TinyLlama via llama.cpp)

While it's not on the level of ChatGPT, it runs entirely on a single-board computer with no licensing fees ‚Äì and that's the point. Feeding the databases with more accurate, real-world information will improve results further.

There's also no reason this has to stay local. The same concept could be deployed on **SAP BTP**, making it accessible to a wider audience while keeping all data within your own tenant.

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äì see [LICENSE](LICENSE) for details.

The TinyLlama model is licensed under the **Apache License 2.0**. Please refer to the [model card on Hugging Face](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF) for details.

---

## üè∑Ô∏è Topics

`sap` `sap-integration-suite` `sap-cpi` `sap-btp` `raspberry-pi` `llm` `local-ai` `tinyllama` `chromadb` `fastapi` `rag` `python`
